
### GLove

<a href='https://zhuanlan.zhihu.com/p/42073620' target='_blank'>参考链接</a>

简单的说，Glove就是用了语料的全局特征和局部特征。


### FastText

<a href='https://zhuanlan.zhihu.com/p/32965521' target='_blank'>参考链接</a>

word2vec把语料库中的每个单词当成原子的，它会为每个单词生成一个向量。这忽略了单词内部的形态特征，比如：“apple” 和“apples”，“达观数据”和“达观”，这两个例子中，两个单词都有较多公共字符，即它们的内部形态类似，但是在传统的word2vec中，这种单词内部形态信息因为它们被转换成不同的id丢失了。

为了克服这个问题，fastText使用了字符级别的n-grams来表示一个单词。对于单词“apple”，假设n的取值为3，则它的trigram有

其中，<表示前缀，>表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，我们可以用这5个trigram的向量叠加来表示“apple”的词向量。

这带来两点好处：

- 对于低频词生成的词向量效果会更好。因为它们的n-gram可以和其它词共享。
- 对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级n-gram向量。

fastText的核心思想就是：将整篇文档的词变成n-gram向量，之后对这些向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。

这里的数据集是有类别的，但是这个类别可能不是那么的清楚，比如说分成健康，旅游，教育，文化，财经之类的几种非常打的类别。

### Word2Vec

- 有没有使用自己的数据训练过Word2vec，详细说一下过程。包括但是不限于：语料如何获取，清理以及语料的大小，超参数的选择及其原因，词表以及维度大小，训练时长等等细节点。
- Word2vec模型是如何获得词向量的？聊一聊你对词嵌入的理解？如何理解分布式假设？
- **如何评估训练出来的词向量的好坏**

word2vec是一种无监督学习算法，没有什么好的评价指标。如果非要用评估，可以使用聚类，可视化的方式，向量的加减，向量相似度等一些方法。

- Word2vec模型如何做到增量训练
- 大致聊一下 word2vec这个模型的细节，包括但不限于：两种模型以及两种优化方法（大致聊一下就可以，下面会详细问）
- 解释一下 hierarchical softmax 的流程(CBOW and Skip-gram)
- 基于6，可以展开问一下模型如何获取输入层，有没有隐层，输出层是什么情况。
- 基于6，可以展开问输出层为何选择霍夫曼树，它有什么优点，为何不选择其他的二叉树
- **基于6，可以问该模型的复杂度是多少，目标函数分别是什么，如何做到更新梯度（尤其是如何更新输入向量的梯度）**

复杂度是$O(log_2 n)$


- **基于6，可以展开问一下 hierarchical softmax 这个模型 有什么缺点**


Hierarchical softmax的缺点就是:虽然我们使用huffman树代替传统的神经网络，可以提高模型训练的效率，但是如果我们训练样本中的中心词w是一个很生僻的词，那么就需要沿着huffman树往下走很多，因为越是高频的词，越是靠近根节点


- 聊一下负采样模型优点（为什么使用负采样技术）
- 如何对输入进行负采样（负采样的具体实施细节是什么）
- **负采样模型对应的目标函数分别是什么（CBOW and Skip-gram）**
- **CBOW和skip-gram相较而言，彼此相对适合哪些场景**
- 有没有使用Word2vec计算过句子的相似度，效果如何，有什么细节可以分享出来


- word2vec中缺少的单词怎么处理
<a href='' target='_blank'>参考链接</a>

1. unk技巧
   在训练word2vec之前，预留一个<unk>符号，把所有stopwords或者低频词都替换成unk，之后使用的时候，也要保留一份词表，对于不在word2vec词表内的词先替换为unk。
   
2. subword技巧
   这个技巧出自fasttext，简而言之就是对oov词进行分词，分词之后再查找，找到的就保留，找不到的继续分词，直到最后分到字级别，肯定是可以找到的对应字向量的。
   
3. BPE技巧
   BPE(byte pair encoder)，字节对编码，也可以叫做digram coding双字母组合编码。BPE首先把一个完整的句子分割为单个的字符，频率最高的相连字符对合并以后加入到词表中，直到达到目标词表大小。对测试句子采用相同的subword分割方式。BPE分割的优势是它可以较好的平衡词表大小和需要用于句子编码的token数量。BPE的缺点在于，它不能提供多种分割的概率。
