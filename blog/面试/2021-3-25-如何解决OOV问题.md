[TOC]

<a href='https://www.zhihu.com/question/55172758/answer/579240329' target='_blank'>参考链接</a>

对于普通的应用，推荐从【数据】的角度来解决OOV的问题。比起更换更复杂的字符级模型，对数据的处理可操作性更强效果也是特别直观地好。另外，如果直接替换成<UNK>标签来忽略OOV的问题，做实际应用的时候不一定好————总不能给用户输出一个<UNK>吧？



### 1.Mixed Word/Character Model

即把所有的OOV词，拆成字符。比如 Jessica，变成\<B>J，\<M>e，\<M>s，\<M>s，\<M>i，\<M>c，\<E>a。其中\<B>\<M>\<E>是Begin，Middle，End的标记。这样处理的好处就是消灭了全部的OOV。坏处就是文本序列变得非常长，对于性能敏感的系统，这是难以接受的维度增长。

### 2. Wordpiece Model（WPM）

和上面一样，同样要进行拆词。不同的是，非OOV的词也要拆，并且非字符粒度，而是sub-word。还是 Jessica，变成\<B>Je，\<M>ssi，\<E>ca。这类方法最早应用于Google的语音识别系统，现在已经在NLP中遍地开花了。拆词规则可以从语料中自动统计学习到，常用的是BPE（Byte Pair Encode）编码，出处在<a href='https://arxiv.org/abs/1508.07909' target='_blank'>《Neural Machine Translation of Rare Words with Subword Units》</a>。和第一种方法相比，虽然序列的长度控制住了，但是在有限词表的情况下，OOV仍然存在。另外，sub-word的OOV有一种麻烦，对于Jessica的例子，即使只有\<M>ssi是OOV，\<B>Je和\<E>ca都在词表内，整个Jessica的单词仍然无法正确表示。

### 3. UNK处理

在训练数据充足的情况下，RNN模型可以轻松支持30k-80k的词表。在大多数情况下，扩大词表都是首选的方案。经过WPM处理后，配合词表加大，剩下的OOV都是冷门的长尾词。如果你不关注这部分性能，可以直接扔掉OOV词，删掉包含OOV的数据。对于分类型任务，就全部替换成\<UNK>标签。对于生成型任务，有不同的细节处理方案，可以看下经典的<a href='https://arxiv.org/abs/1410.8206' target='_blank'>《Addressing the Rare Word Problem in Neural Machine Translation》</a>，里面介绍了Copyable、PosALL和PosUNK三种替换策略。这类策略对于实体类NER的词，有比较好的效果。

### 4. 中文的处理

英文中包含的姓名、复合词、同源词和外来词，使得WPM的方法效果拔群。在处理中文时，WPM可以有效帮助解决混入的英文词和阿拉伯数字等。对于纯中文的句子，分割成子词的意义并不大。这时候，扩大词表仍然是首选。


### 5. 扩大词表

终极解决办法。通常情况不使用大词表，一方面是因为训练数据的多样性有限，另一方面是softmax的计算速度受限。对于第一种情况，扩大语料范围。对于第二种情况，相关的加速策略可以将词表扩大10倍而GPU上的预测速度只降低一半（从5W词到50W词）。比如《On Using Very Large Target Vocabulary for Neural Machine Translation》。tensorflow中有对应的实现 tf.nn.sampled_softmax_loss
