
<a href='https://blog.csdn.net/tyhj_sf/article/details/79932893' traget='_balnk'>常见激活函数</a>

### GELU

<a href='https://arxiv.org/abs/1606.08415' traget='_balnk'>论文链接</a>
<a href='https://www.cnblogs.com/shiyublog/p/11121839.html#_label3' traget='_balnk'>参考博客</a>
<a href='https://wenku.baidu.com/view/1921ddb9647d27284a735139.html' traget='_balnk'>常见泰勒展开</a>
<a href='https://www.cnblogs.com/htj10/p/8621771.html' traget='_balnk'>高斯误差函数</a>
<a href='https://baike.baidu.com/item/%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0/5890875?fr=aladdin#4' traget='_balnk'>高斯误差函数及泰勒展开</a>

## Motivation

&emsp;&emsp;GELU非线性的实现是通过对输入进行随机的正则化变化，就是对输入随机的乘上0和1。但是这个随机值是和输入有关的。同时以往的非线性激活函数和随机正则化是不相关的，但是GELU将两者进行了结合。

## 激活函数具体形式

&emsp;&emsp;GELU是让输入x乘以一个服从伯努利分布的m，但是m的分布又依赖于输入的x

$$
\begin{aligned}
& m \sim Bernoulli(\Phi(x)) \\
& \Phi(x) = P(X \leqslant x) \\
& X \sim N(0,1)
\end{aligned}
$$

&emsp;&emsp;于是就得到了下面的GELU的公式:

$$
\begin{aligned}
GELU(X) &= \Phi(x) * x * 1 + (1 - \Phi(x)) * 0 * x \\
&= x \ Phi(x) \\
&约等于的话就是: \\
&= 0.5 x (1 + tanh[\sqrt{2 / \pi}(x+0.044715 x^3)])
\end{aligned}
$$

### 激活函数求导

- sigmoid函数求导：
  $$
  f'(x) = (\frac{1}{1 + e^{-x}})' = f(x)(1- f(x))
  $$

- tanh函数求导
  $$
  f'(x) = (\frac{e^x - e^{-x}}{e^{x} + e^{-x}})' = 1 - f(x)^2
  $$