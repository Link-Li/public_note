<a href='https://zhuanlan.zhihu.com/p/33173246' target='_blank'>总结的非常详细</a>

### 为什么需要归一化

因为会产生源数据和目标空间数据分布不一致的情况，也就导致了每个神经元的输入数据不再是独立同分布的
- 上层参数需要不断适应新的输入数据的分布，降低了学习速度
- 下层的输入的变化可能趋向于变大或者变小，导致上层落入饱和区，造成学习过早停止
- 每层的更新都会影响其他层，因为此每层的参数更新变化就更加敏感

### layernorm和batch norm的差别

- 简单来说，bn是在整个batch上面进行的，每次进行归一化的是整个batch上的一维，所以bn的计算和batch是紧密相连的；因此bn比较适用于一个batch比较大，而且数据分布比较接近。
- 而layernorm是在数据上面进行的归一化，所以ln就可以适用于rnn这样的网络，但是ln对于一个数据中的不同特征（特别是不相似的特征）进行了归一化，这样会降低模型的表达能力
