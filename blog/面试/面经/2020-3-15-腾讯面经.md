#### 面试ailab一面

- 样本不平衡
- Focal Loss和GHM Loss
- 图文预训练

- 题目：
给定一个排序二叉树， 给定两个子节点的值，问这两个子节点最近的距离：

思路：先找这两个子节点的公共祖先，然后再去找这个公共祖先下面的这两个点的深度，最后两个深度相加-1即可：

```
#coding=utf-8
import sys 
#str = input()
#print(str)

class TreeNode:
    def __init__(self, val):
        self.val = val
        self.left = None
        self.right = None

class Solution:
    def find_short(self, root, p, q):
        def dfs(root, count, node):
            if not root:
                return count
            result = count
            if root.val > node.val:
                result = dfs(root.left, count+1, node)
            elif root.val < node.val:
                result = dfs(root.right, count+1, node)
            return result
        
        if p.val == q.val:
            return 0
        while root:
            if root.val < q.val and root.val < p.val:
                root = root.right
            elif root.val > q.val and root.val > p.val:
                root = root.left
            else:
                break
        left_count = 0
        right_count = 0
        if p.val > root.val:
            right_count = dfs(root.right, 1, p)
            left_count = dfs(root.lef, 1, q)
        elif p.val <= root.val:
            right_count = dfs(root.right, 1, q)
            left_count = dfs(root.lef, 1, p)
        return right_count + left_count - 1
         
```


#### 腾讯二面：

- SVM的损失函数是什么（合页损失函数）
- 详细介绍Transformer的各个部分
  - 输入是什么样子的
  - 有几种attention计算（self-multi-att，self-mask-multi-att， multi-att）
  - decoder的输入是encoder的哪一层的输出（最后一层的）
  - 详细介绍内部的各个计算过程
- 详细介绍一下BERT, RoBERTa，ALBERT，XLNet的区别
- word2vec：
  - 为什么层次softmax，原来来的softmax有什么缺点，计算复杂度度从多少降到了多少（从O（n）降到了O（$log_2n$））
  - 负采样的复杂度变成了多少（变成了常数）
- 对比学习
- 双线性池化
- 都有那些损失函数
- 比赛：为啥最终使用了单模态信息
- 怎么处理OOV问题

题目：
1.模拟dropout
2.二维的滑动窗口最大值:https://leetcode-cn.com/problems/sliding-window-maximum/