### 不可导梯度传播

<a href='https://blog.csdn.net/weixin_42720875/article/details/105936398' target='_blank'>激活函数不可导，池化不可导</a>

<a href='https://blog.csdn.net/oBrightLamp/article/details/84105097' traget='_blank'>dropout不可导</a>


### L1正则和L2正则的区别

<a href='https://www.zhihu.com/question/26485586' target='_blank'>参考链接</a>

简单来说，L1和L2具有以下的一些不同：
- L2计算起来更加方便，特别是求导方面。L1的导数就不是连续的
- L1更加偏向于输出稀疏的特征，会把不重要的特征变成0，所以L1也是一个特征筛选器。出现这样的情况的原因是因为L1的导数不是1，就是-1，这样导致每次更新梯度的时候，L1都是稳步向0前进的。二L2并不会，因为L2正则的导数在参数接近0的时候也接近0，就导致更新幅度越来越小了。
- 接上一条，因为这些特征，导致L2对大数的惩罚比较厉害，而L1就是比较稀疏了。

### NLP中的subword算法及实现(BPE)


<a href='https://zhuanlan.zhihu.com/p/112444056' target='_blank'>参考链接</a>

<a href='2021-3-25-如何解决OOV问题.md' target='_blank'>2021-3-25-如何解决OOV问题.md</a>

是为了解决OOV问题的，因为以往的解决办法是给词典增加词汇量，但是过多的词汇会导致一些问题：
- 稀疏问题: 某些词汇出现的频率很低，得不到充分的训练
- 计算量问题: 词典过大，也就意味着embedding过程的计算量会变大

为了处理这个问题，一个思路是将字符当做基本单元，建立character-level模型。character-level模型试图使用26个字母加上一些符号去表示所有的词汇，相比于word-level模型，这种处理方式的粒度变小，其输入长度变长，使得数据更加稀疏并且难以学习长远程的依赖关系。

word-level模型导致严重的OOV，而character-level模型粒度又太小，那么subword-level的处理方式就应运而生。subword将单词划分为更小的单元，比如"older"划分为"old" 和 "er"


