<a href='https://www.cnblogs.com/LXP-Never/p/13755354.html' target='_blank'>参考链接1</a>

<a href='https://zhuanlan.zhihu.com/p/56475281' target='_blank'>参考链接2</a>

<a href='' target='_blank'>参考链接3</a>

### 出现过拟合的原因

- 模型复杂度过高，参数过多
- 训练数据比较小
- 训练集和测试集分布不一致:
  - 样本里面的噪声数据干扰过大，导致模型过分记住了噪声特征，反而忽略了真实的输入输出特征
  - 训练集和测试集特征分布不一样（如果训练集和测试集使用了不同类型的数据集会出现这种情况）


### 解决方案

#### 1.降低模型复杂度

可以简单地移除层或者减少神经元的数量使得网络规模变小。与此同时，计算神经网络中不同层的输入和输出维度也十分重要。虽然移除层的数量或神经网络的规模并无通用的规定，但如果神经网络发生了过拟合，就尝试缩小它的规模。

#### 2.增加更多的数据

#### 3.数据增强

这可以帮助我们增加数据集规模从而减少过拟合。因为随着数据量的增加，模型无法过拟合所有样本，因此不得不进行泛化。计算机视觉领域通常的做法有：翻转、平移、旋转、缩放、改变亮度、添加噪声等等，音频数据增强方法有：增加噪音、增加混响、时移、改变音调和时间拉伸

#### 4.正则化

损失函数分为经验风险损失函数和结构风险损失函数，结构风险损失函数就是经验损失函数+表示模型复杂度的正则化，正则项通常选择L1或者L2正则化。结构风险损失函数能够有效地防止过拟合。

$L_1$ 正则化是指权值向量$w$中各个元素的绝对值之和，通常表示为 $||w||_1$ ， $L_1$ 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择，一定程度上， $L_1$也可以反之过拟合
$L_2$ 正则化是指权值向量中各个元素的平方和的平方，通常表示为 $||w||_2$ ， $L_2$ 正则化可以防止模型过拟合
那 $L_1$ 和 $L_2$ 正则化是如何防止过拟合呢？首先我们先明白稀疏参数和更小参数的好处。

稀疏参数（ $L_1$ ）：参数的稀疏，在一定程度实现了特征的选择。稀疏矩阵指有很多元素为0，少数参数为非零值。一般而言，只有少部分特征对模型有贡献，大部分特征对模型没有贡献或者贡献很小，稀疏参数的引入，使得一些特征对应的参数是0，所以就可以剔除可以将那些没有用的特征，从而实现特征选择。

更小参数（ $L_2$ ): 越复杂的模型，越是尝试对所有样本进行拟合，那么就会造成在较小的区间中产生较大的波动，这个较大的波动反映出在这个区间内的导数就越大。只有越大的参数才可能产生较大的导数。试想一下，参数大的模型，数据只要偏移一点点，就会对结果造成很大的影响，但是如果参数比较小，数据的偏移对结果的影响力就不会有什么影响，那么模型也就能够适应不同的数据集，也就是泛化能力强，所以一定程度上避免过拟合。

#### 5.dropout


#### 6.提前停止训练


#### 7.清洗数据

将一些异常的数据清除


#### 8.使用集成学习

这里应该使用bagging方式，因为boosting方式可能会导致过拟合，因为boosting的方式只是减少偏差，而不是减少方差(不确定，最好别提这个)