# 如何在pytorch中使用word2vec训练好的词向量

```
torch.nn.Embedding()
```
&emsp;&emsp;这个方法是在pytorch中将词向量和词对应起来的一个方法. 一般情况下,如果我们直接使用下面的这种:
```
self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeding_dim)
num_embeddings=vocab_size   表示词汇量的大小
embedding_dim=embeding_dim  表示词向量的维度
```

&emsp;&emsp;这种情况下, 因为没有指定训练好的词向量, 所以embedding会帮咱们生成一个随机的词向量(但是在我刚刚测试的一个情感二分类问题中, 我发现好像用不用预训练的词向量, 结果差不多, 不过不排除是因为当时使用的模型比较简单, 导致一些特征根本就没提取出来). 

&emsp;&emsp;如果我想使用word2vec预训练好的词向量该怎么做呢?
&emsp;&emsp;其实很简单,pytorch已经给我们提供好了接口

```
self.embedding.weight.data.copy_(torch.from_numpy(embeding_vector))
self.embedding.weight.requires_grad = False
```

&emsp;&emsp;上面两句代码的意思, 第一句就是导入词向量, 第二句表示的是在反向传播的时候, 不要对这些词向量进行求导更新. 我还看到有人会在优化器那里使用这样的代码:

```
# emotion_net是我定义的模型
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, emotion_net.parameters()), lr=1e-3, betas=(0.9, 0.99))
```
&emsp;&emsp;大概意思也是为了保证词向量不会被反向传播而更新, 具体有没有用我就不清楚了.

&emsp;&emsp;**其实我感觉大家比较在意的其实应该是`embeding_vector`的形式, 下面我就介绍一下`embeding_vector`的形式**

为了讲述方便, 这里定义出下面几个矩阵

$$
\begin {aligned}
embeding\_vector &:  表示词向量, 每行是一个词的词向量, 有多少行就说明有多少单词 \\
word\_list &: 表示单词列表, 里面就是单词 \\
word\_to\_index &: 这个矩阵将word\_list中的单词和  embeding\_vector中的位置对应起来
\end {aligned}
$$


&emsp;&emsp;其实`embeding_vector`是一个numpy矩阵, 当然你看到了, 实际输入到pytorch的时候, 是需要转换成tensor类型的. 这个矩阵是什么样子的呢? 其中这个矩阵是 $[vocab\_size \times embeding\_dim]$ 的形式. 其中一共包含$vocab\_size$ 个单词, 每个单词的维度是 $embed\_dim$, 我们把这样一个矩阵输入就行了.

&emsp;&emsp;之后, 我们要做的其实就是将 $word\_to\_index$ 这个矩阵搞出来, 这里的单词转下标的矩阵, 就是联系 $embeding\_vector$ 和 $word\_list$ 这两个矩阵的中间者. 我们在输入到` torch.nn.Embedding`中之前, 需要先通过 $word\_to\_index$ 将单词转换成 $embeding\_vector$ 的下标就可以了.
